# JupyterHub spawn config
apiVersion: v1
kind: ConfigMap
metadata:
  name: jupyterhub-config
data:
  jupyterhub-config.py: |
    import os,sys

    c = get_config()

    c.JupyterHub.spawner_class='kubespawner.KubeSpawner'
    c.KubeSpawner.start_timeout=1000

    # Which container to spawn
    c.KubeSpawner.image_spec='labshare/polyglot-notebook:0.5.3'
    c.KubeSpawner.default_url = '/lab'
    c.KubeSpawner.uid = 1000 #uid 1000 corresponds to jovyan, uid 0 to root
    c.KubeSpawner.cmd = ['jupyter-labhub']
    c.KubeSpawner.working_dir = '/home/jovyan'
    c.KubeSpawner.service_account='jupyteruser-sa'

    # Per-user storage configuration
    c.KubeSpawner.pvc_name_template = 'claim-{username}'
    c.KubeSpawner.storage_capacity = '1Gi'
    c.KubeSpawner.storage_access_modes = ['ReadWriteOnce']
    c.KubeSpawner.storage_pvc_ensure = True

    # Volumes to attach to Pod
    c.KubeSpawner.volumes = [
      {
        'name': 'volume-{username}',
        'persistentVolumeClaim': {
          'claimName': 'claim-{username}'
        }
      },
      {
        'name': 'shared-volume',
        'persistentVolumeClaim': {
          'claimName': 'notebooks-pv-claim'
        }
      },
      {
        'name': 'wipp-volume',
        'persistentVolumeClaim': {
          'claimName': 'wipp-pv-claim'
        }
      }
    ]

    # Where to mount volumes
    c.KubeSpawner.volume_mounts = [
      {
        'mountPath': '/home/jovyan/work',
        'name': 'volume-{username}'
      },
      {
        'mountPath': '/opt/shared/notebooks',
        'name': 'shared-volume'
      },
      {
        'mountPath': '/opt/shared/wipp',
        'name': 'wipp-volume'
      }
    ]

    c.JupyterHub.allow_named_servers=True
    c.JupyterHub.ip='0.0.0.0'
    c.JupyterHub.hub_ip='0.0.0.0'

    # Required for AWS
    c.JupyterHub.hub_connect_ip='jupyterhub-internal'

    c.JupyterHub.cleanup_servers=False
    # c.ConfigurableHTTPProxy.should_start=False
    c.JupyterHub.cookie_secret_file = '/srv/jupyterhub/jupyterhub_cookie_secret'

    c.JupyterHub.authenticator_class = 'dummy'

    # Set up WIPP UI url for integration with WIPP
    c.KubeSpawner.environment = {
        'WIPP_UI_URL': 'http://localhost:32001'
    }

    # Service to shutdown inactive Notebook servers after --timeout seconds
    c.JupyterHub.services = [
        {
            'name': 'cull-idle',
            'admin': True,
            'command': [sys.executable, '/srv/jupyterhub/config/cull-idle-servers.py', '--timeout=36000'],
        }
    ]
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cull-idle-servers
data:
  cull-idle-servers.py: |
    import json
    import os
    from datetime import datetime
    from datetime import timezone
    from functools import partial

    try:
        from urllib.parse import quote
    except ImportError:
        from urllib import quote

    import dateutil.parser

    from tornado.gen import coroutine, multi
    from tornado.locks import Semaphore
    from tornado.log import app_log
    from tornado.httpclient import AsyncHTTPClient, HTTPRequest
    from tornado.ioloop import IOLoop, PeriodicCallback
    from tornado.options import define, options, parse_command_line


    def parse_date(date_string):
        """Parse a timestamp
        If it doesn't have a timezone, assume utc
        Returned datetime object will always be timezone-aware
        """
        dt = dateutil.parser.parse(date_string)
        if not dt.tzinfo:
            # assume na√Øve timestamps are UTC
            dt = dt.replace(tzinfo=timezone.utc)
        return dt


    def format_td(td):
        """
        Nicely format a timedelta object
        as HH:MM:SS
        """
        if td is None:
            return "unknown"
        if isinstance(td, str):
            return td
        seconds = int(td.total_seconds())
        h = seconds // 3600
        seconds = seconds % 3600
        m = seconds // 60
        seconds = seconds % 60
        return "{h:02}:{m:02}:{seconds:02}".format(h=h, m=m, seconds=seconds)


    @coroutine
    def cull_idle(
        url, api_token, inactive_limit, cull_users=False, max_age=0, concurrency=10
    ):
        """Shutdown idle single-user servers
        If cull_users, inactive *users* will be deleted as well.
        """
        auth_header = {'Authorization': 'token %s' % api_token}
        req = HTTPRequest(url=url + '/users', headers=auth_header)
        now = datetime.now(timezone.utc)
        client = AsyncHTTPClient()

        if concurrency:
            semaphore = Semaphore(concurrency)

            @coroutine
            def fetch(req):
                """client.fetch wrapped in a semaphore to limit concurrency"""
                yield semaphore.acquire()
                try:
                    return (yield client.fetch(req))
                finally:
                    yield semaphore.release()

        else:
            fetch = client.fetch

        resp = yield fetch(req)
        users = json.loads(resp.body.decode('utf8', 'replace'))
        futures = []

        @coroutine
        def handle_server(user, server_name, server):
            """Handle (maybe) culling a single server
            Returns True if server is now stopped (user removable),
            False otherwise.
            """
            log_name = user['name']
            if server_name:
                log_name = '%s/%s' % (user['name'], server_name)
            if server.get('pending'):
                app_log.warning(
                    "Not culling server %s with pending %s", log_name, server['pending']
                )
                return False

            # jupyterhub < 0.9 defined 'server.url' once the server was ready
            # as an *implicit* signal that the server was ready.
            # 0.9 adds a dedicated, explicit 'ready' field.
            # By current (0.9) definitions, servers that have no pending
            # events and are not ready shouldn't be in the model,
            # but let's check just to be safe.

            if not server.get('ready', bool(server['url'])):
                app_log.warning(
                    "Not culling not-ready not-pending server %s: %s", log_name, server
                )
                return False

            if server.get('started'):
                age = now - parse_date(server['started'])
            else:
                # started may be undefined on jupyterhub < 0.9
                age = None

            # check last activity
            # last_activity can be None in 0.9
            if server['last_activity']:
                inactive = now - parse_date(server['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'started' field which is never None
                # for running servers
                inactive = age

            should_cull = (
                inactive is not None and inactive.total_seconds() >= inactive_limit
            )
            if should_cull:
                app_log.info(
                    "Culling server %s (inactive for %s)", log_name, format_td(inactive)
                )

            if max_age and not should_cull:
                # only check started if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling server %s (age: %s, inactive for %s)",
                        log_name,
                        format_td(age),
                        format_td(inactive),
                    )
                    should_cull = True

            if not should_cull:
                app_log.debug(
                    "Not culling server %s (age: %s, inactive for %s)",
                    log_name,
                    format_td(age),
                    format_td(inactive),
                )
                return False

            if server_name:
                # culling a named server
                delete_url = url + "/users/%s/servers/%s" % (
                    quote(user['name']),
                    quote(server['name']),
                )
            else:
                delete_url = url + '/users/%s/server' % quote(user['name'])

            req = HTTPRequest(url=delete_url, method='DELETE', headers=auth_header)
            resp = yield fetch(req)
            if resp.code == 202:
                app_log.warning("Server %s is slow to stop", log_name)
                # return False to prevent culling user with pending shutdowns
                return False
            return True

        @coroutine
        def handle_user(user):
            """Handle one user.
            Create a list of their servers, and async exec them.  Wait for
            that to be done, and if all servers are stopped, possibly cull
            the user.
            """
            # shutdown servers first.
            # Hub doesn't allow deleting users with running servers.
            # jupyterhub 0.9 always provides a 'servers' model.
            # 0.8 only does this when named servers are enabled.
            if 'servers' in user:
                servers = user['servers']
            else:
                # jupyterhub < 0.9 without named servers enabled.
                # create servers dict with one entry for the default server
                # from the user model.
                # only if the server is running.
                servers = {}
                if user['server']:
                    servers[''] = {
                        'last_activity': user['last_activity'],
                        'pending': user['pending'],
                        'url': user['server'],
                    }
            server_futures = [
                handle_server(user, server_name, server)
                for server_name, server in servers.items()
            ]
            results = yield multi(server_futures)
            if not cull_users:
                return
            # some servers are still running, cannot cull users
            still_alive = len(results) - sum(results)
            if still_alive:
                app_log.debug(
                    "Not culling user %s with %i servers still alive",
                    user['name'],
                    still_alive,
                )
                return False

            should_cull = False
            if user.get('created'):
                age = now - parse_date(user['created'])
            else:
                # created may be undefined on jupyterhub < 0.9
                age = None

            # check last activity
            # last_activity can be None in 0.9
            if user['last_activity']:
                inactive = now - parse_date(user['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'created' field which is never None
                inactive = age

            should_cull = (
                inactive is not None and inactive.total_seconds() >= inactive_limit
            )
            if should_cull:
                app_log.info("Culling user %s (inactive for %s)", user['name'], inactive)

            if max_age and not should_cull:
                # only check created if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling user %s (age: %s, inactive for %s)",
                        user['name'],
                        format_td(age),
                        format_td(inactive),
                    )
                    should_cull = True

            if not should_cull:
                app_log.debug(
                    "Not culling user %s (created: %s, last active: %s)",
                    user['name'],
                    format_td(age),
                    format_td(inactive),
                )
                return False

            req = HTTPRequest(
                url=url + '/users/%s' % user['name'], method='DELETE', headers=auth_header
            )
            yield fetch(req)
            return True

        for user in users:
            futures.append((user['name'], handle_user(user)))

        for (name, f) in futures:
            try:
                result = yield f
            except Exception:
                app_log.exception("Error processing %s", name)
            else:
                if result:
                    app_log.debug("Finished culling %s", name)


    if __name__ == '__main__':
        define(
            'url',
            default=os.environ.get('JUPYTERHUB_API_URL'),
            help="The JupyterHub API URL",
        )
        define('timeout', default=600, help="The idle timeout (in seconds)")
        define(
            'cull_every',
            default=0,
            help="The interval (in seconds) for checking for idle servers to cull",
        )
        define(
            'max_age',
            default=0,
            help="The maximum age (in seconds) of servers that should be culled even if they are active",
        )
        define(
            'cull_users',
            default=False,
            help="""Cull users in addition to servers.
                    This is for use in temporary-user cases such as tmpnb.""",
        )
        define(
            'concurrency',
            default=10,
            help="""Limit the number of concurrent requests made to the Hub.
                    Deleting a lot of users at the same time can slow down the Hub,
                    so limit the number of API requests we have outstanding at any given time.
                    """,
        )

        parse_command_line()
        if not options.cull_every:
            options.cull_every = options.timeout // 2
        api_token = os.environ['JUPYTERHUB_API_TOKEN']

        try:
            AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
        except ImportError as e:
            app_log.warning(
                "Could not load pycurl: %s\n"
                "pycurl is recommended if you have a large number of users.",
                e,
            )

        loop = IOLoop.current()
        cull = partial(
            cull_idle,
            url=options.url,
            api_token=api_token,
            inactive_limit=options.timeout,
            cull_users=options.cull_users,
            max_age=options.max_age,
            concurrency=options.concurrency,
        )
        # schedule first cull immediately
        # because PeriodicCallback doesn't start until the end of the first interval
        loop.add_callback(cull)
        # schedule periodic cull
        pc = PeriodicCallback(cull, 1e3 * options.cull_every)
        pc.start()
        try:
            loop.start()
        except KeyboardInterrupt:
            pass
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jupyterhub-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: jupyterhub-role
rules:
  - apiGroups: [""]
    resources: ["*"]
    verbs: ["get", "list", "watch", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: jupyterhub-rb
subjects:
  - kind: ServiceAccount
    name: jupyterhub-sa
roleRef:
  kind: Role
  name: jupyterhub-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jupyteruser-sa
---
# PVC for Notebooks
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: notebooks-pv-claim
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jupyterhub
  labels:
    application: jupyterhub
spec:
  selector:
    matchLabels:
      application: jupyterhub
  replicas: 1
  template:
    metadata:
      labels:
        application: jupyterhub
    spec:
      serviceAccountName: jupyterhub-sa
      containers:
        - name: jupyterhub
          image: labshare/jupyterhub:0.3.2
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
              # This name will be used in the Service.
              name: jupyter-http
            - containerPort: 8081
              # This name will be used in the Service.
              name: jupyter-in
          command: [
            "jupyterhub",
            "-f",
            "/srv/jupyterhub/config/jupyterhub-config.py",
            "--debug"
          ]
          volumeMounts:
            - mountPath: /srv/jupyterhub/config/jupyterhub-config.py
              name: jupyterhub-config
              subPath: jupyterhub-config.py
            - mountPath: /srv/jupyterhub/config/cull-idle-servers.py
              name: cull-idle-servers
              subPath: cull-idle-servers.py
      volumes:
        - name: jupyterhub-config
          configMap:
              name: jupyterhub-config
              items:
              - key: jupyterhub-config.py
                path: jupyterhub-config.py
        - name: cull-idle-servers
          configMap:
              name: cull-idle-servers
              items:
              - key: cull-idle-servers.py
                path: cull-idle-servers.py
---
# JupyterHub services
# External JupyterHub UI Service definition
apiVersion: v1
kind: Service
metadata:
  name: jupyterhub
  namespace: default
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 32003
    name: http
    targetPort: jupyter-http
  selector:
    application: jupyterhub
---
# Internal JupyterHub API Service definition
apiVersion: v1
kind: Service
metadata:
  name: jupyterhub-internal
spec:
  ports:
  - port: 8081
    name: http
    # Use named container port.
    targetPort: jupyter-in
  selector:
    application: jupyterhub
---
